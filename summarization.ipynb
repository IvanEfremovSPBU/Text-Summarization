{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Remix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Remix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Remix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Remix\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    lemmatized_sentences = []\n",
    "    all_lemmas = set()\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        words = word_tokenize(sentence.lower())\n",
    "        \n",
    "        lemmas = []\n",
    "        for word in words:\n",
    "            if word.isalnum() and word not in stop_words:\n",
    "                lemmas.append(lemmatizer.lemmatize(word))\n",
    "                \n",
    "        lemmatized_sentences.append(lemmas)\n",
    "        all_lemmas.update(lemmas)\n",
    "        \n",
    "    return lemmatized_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists find the biggest coral ever. It is in the Pacific Ocean. The coral is 34 meters long, bigger than a blue whale.\n",
      "\n",
      "It may be over 300 years old. Scientists find it during a trip to study the ocean and climate change. They first think that it is a shipwreck. There is a big meeting about climate change right now. Small countries worry because warming oceans kill coral reefs. But this coral is still alive. Scientists want to study it to learn why it does not die. There is hope that not all coral reefs will die.\n"
     ]
    }
   ],
   "source": [
    "with open('text.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    \n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Экстрактивные методы суммаризации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Алгоритм Луна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def luhn_summarizer(text, summarized_length, threshold_factor=1.5):\n",
    "    lemmatized_sentences = preprocess_text(text)\n",
    "\n",
    "    words_combined = [word for sentence in lemmatized_sentences for word in sentence]\n",
    "    words_freqences = Counter(words_combined)\n",
    "    \n",
    "    avg_words_freq = sum(words_freqences.values()) / len(words_freqences)\n",
    "    keywords = []\n",
    "    \n",
    "    for word, count in words_freqences.items():\n",
    "        if count >= avg_words_freq * threshold_factor:\n",
    "            keywords.append(word)\n",
    "    \n",
    "    sentences = sent_tokenize(text)\n",
    "    sentence_scores = {}  \n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        \n",
    "        sentence_lemmas = lemmatized_sentences[i]\n",
    "        \n",
    "        start_index = 0\n",
    "        max_score = 0\n",
    "\n",
    "        keywords_indices = [i for i, word in enumerate(sentence_lemmas) if word in keywords]\n",
    "        \n",
    "        for i in keywords_indices:\n",
    "            keywords_in_interval = 0\n",
    "            interval_size = 0\n",
    "            \n",
    "            for j in range(start_index, i+1):\n",
    "                if sentence_lemmas[j] in keywords:\n",
    "                    keywords_in_interval += 1\n",
    "                \n",
    "                interval_size += 1\n",
    "                \n",
    "                if interval_size - keywords_in_interval > 4:\n",
    "                    break\n",
    "                \n",
    "            if interval_size >0: # предотвращение ошибки деления на 0\n",
    "                score = (keywords_in_interval**2) / interval_size\n",
    "                max_score = max(max_score, score)\n",
    "\n",
    "            start_index = i + 1\n",
    "            \n",
    "            sentence_scores[sentence] = max_score\n",
    "\n",
    "    sorted_sentences = sorted(sentence_scores.items(), key=lambda item: item[1], reverse=True)\n",
    "    summary = \" \".join([sentence for sentence, score in sorted_sentences[:summarized_length]])  \n",
    "        \n",
    "    return summary  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scientists find the biggest coral ever. The coral is 34 meters long, bigger than a blue whale.\n"
     ]
    }
   ],
   "source": [
    "print(luhn_summarizer(text, 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
